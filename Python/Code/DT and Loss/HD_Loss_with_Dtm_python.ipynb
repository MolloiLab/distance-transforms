{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up env\n",
    "- CUDA: 11.3\n",
    "- CUDNN: 8.6.0\n",
    "- Python: 3.9.5\n",
    "- Pytorch: 1.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import glob\n",
    "import h5py\n",
    "import torch\n",
    "import monai\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_gui\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    ToTensord,\n",
    "    Resized,\n",
    "    RandSpatialCropd,\n",
    "    CenterSpatialCropd,\n",
    "    NormalizeIntensityd\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from monai.data import CacheDataset\n",
    "from scipy.ndimage import _nd_image\n",
    "from torch.utils.data import Dataset\n",
    "from monai.config import print_config\n",
    "from monai.networks.layers import Norm\n",
    "from monai.utils import MetricReduction\n",
    "from torch.utils.data import DataLoader\n",
    "from monai.apps import download_and_extract\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.inferers import sliding_window_inference\n",
    "from scipy.ndimage import distance_transform_edt as distance\n",
    "from monai.metrics import compute_meandice, compute_hausdorff_distance\n",
    "from monai.metrics.utils import do_metric_reduction, ignore_background\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not in Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hd_loss_2D(seg_soft, gt, seg_dtm, gt_dtm):\n",
    "    \"\"\"\n",
    "    compute huasdorff distance loss for binary segmentation\n",
    "    input: seg_soft: softmax results,  shape=(x,y)\n",
    "           gt: ground truth, shape=(x,y)\n",
    "           seg_dtm: segmentation distance transform map; shape=(x,y)\n",
    "           gt_dtm: ground truth distance transform map; shape=(x,y)\n",
    "    output: boundary_loss; sclar\n",
    "    \"\"\"\n",
    "\n",
    "    delta_s = (seg_soft - gt) ** 2\n",
    "    s_dtm = seg_dtm ** 2\n",
    "    g_dtm = gt_dtm ** 2\n",
    "    dtm = s_dtm + g_dtm\n",
    "    multipled = torch.einsum('xy, xy->xy', delta_s, dtm)\n",
    "    hd_loss = multipled.mean()\n",
    "\n",
    "    return hd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hd_loss_3D(seg_soft, gt, seg_dtm, gt_dtm):\n",
    "    \"\"\"\n",
    "    compute huasdorff distance loss for binary segmentation\n",
    "    input: seg_soft: softmax results,  shape=(x,y,z)\n",
    "           gt: ground truth, shape=(x,y,z)\n",
    "           seg_dtm: segmentation distance transform map; shape=(x,y,z)\n",
    "           gt_dtm: ground truth distance transform map; shape=(x,y,z)\n",
    "    output: boundary_loss; sclar\n",
    "    \"\"\"\n",
    "\n",
    "    delta_s = (seg_soft - gt) ** 2\n",
    "    s_dtm = seg_dtm ** 2\n",
    "    g_dtm = gt_dtm ** 2\n",
    "    dtm = s_dtm + g_dtm\n",
    "    multipled = torch.einsum('xyz, xyz->xyz', delta_s, dtm)\n",
    "    hd_loss = multipled.mean()\n",
    "\n",
    "    return hd_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hd_loss_2D_loop(seg_soft, gt, seg_dtm, gt_dtm):\n",
    "#     \"\"\"\n",
    "#     compute huasdorff distance loss for binary segmentation\n",
    "#     input: seg_soft: softmax results,  shape=(b,2,x,y,z)\n",
    "#            gt: ground truth, shape=(b,x,y,z)\n",
    "#            seg_dtm: segmentation distance transform map; shape=(b,2,x,y,z)\n",
    "#            gt_dtm: ground truth distance transform map; shape=(b,2,x,y,z)\n",
    "#     output: boundary_loss; sclar\n",
    "#     \"\"\"\n",
    "\n",
    "#     delta_s = (seg_soft - gt) ** 2\n",
    "#     s_dtm = seg_dtm ** 2\n",
    "#     g_dtm = gt_dtm ** 2\n",
    "#     dtm = s_dtm + g_dtm\n",
    "#     multipled = torch.einsum('bcxy, bcxy->bcxy', delta_s, dtm)\n",
    "#     hd_loss = multipled.mean()\n",
    "\n",
    "#     return hd_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare other stuff for timing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_range_2D = np.arange(2.0, 513.0, 21.25)\n",
    "num_range_3D = np.arange(2.0, 129.0, 5.25)\n",
    "test_cases_2D = []\n",
    "test_cases_3D = []\n",
    "sizes_2D = []\n",
    "sizes_3D = []\n",
    "# test_cases_2D_loop = []\n",
    "# test_cases_3D_loop = []\n",
    "# sizes_2D_loop = []\n",
    "# sizes_3D_loop = []\n",
    "for i in num_range_2D:\n",
    "    n = int(i)\n",
    "    _size = n**2\n",
    "    #2D\n",
    "    sizes_2D.append(_size)\n",
    "    # non loop\n",
    "    pred = torch.from_numpy(np.random.rand(n,n).astype(np.float32))\n",
    "    mask = torch.from_numpy(np.random.randint(2, size=(n,n)).astype(np.float32))\n",
    "    test_cases_2D.append([pred, mask])\n",
    "\n",
    "# for n in 16 : 16 : 512\n",
    "#     print(\"2DLoop $n --> \")\n",
    "#     #2D\n",
    "#     push!(sizes_2D_loop, n^2)\n",
    "#     # loop\n",
    "#     x = rand(Float32, n, n, 1, 1)\n",
    "#     mask = Float32.(rand([0, 1], n, n, 1, 1))\n",
    "#     push!(test_cases_2D_loop, deepcopy([x, mask]))\n",
    "# end\n",
    "for i in num_range_3D:\n",
    "    n = int(i)\n",
    "    _size = n**3\n",
    "    #3D\n",
    "    sizes_3D.append(_size)\n",
    "    # non loop\n",
    "    pred = torch.from_numpy(np.random.rand(n,n,n).astype(np.float32))\n",
    "    mask = torch.from_numpy(np.random.randint(2, size=(n,n,n)).astype(np.float32))\n",
    "    test_cases_3D.append([pred, mask])\n",
    "# for n in 16 : 16 : 128\n",
    "#     print(\"3DLoop $n --> \")\n",
    "#     #3D\n",
    "#     push!(sizes_3D_loop, n^3)\n",
    "#     # loop\n",
    "#     x = rand(Float32, n, n, n, 1, 1)\n",
    "#     mask = Float32.(rand([0, 1], n, n, n, 1, 1))\n",
    "#     push!(test_cases_3D_loop, deepcopy([x, mask]))\n",
    "# end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 2D U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(stride, in_dim, out_dim):\n",
    "    return nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=stride, padding=1)\n",
    "\n",
    "def tran2d(stride, in_dim, out_dim):\n",
    "    return nn.ConvTranspose2d(in_dim, out_dim, kernel_size=3, stride=stride, padding=1, output_padding=1)\n",
    "\n",
    "def conv12d(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        conv2d(1, in_dim, out_dim),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.LeakyReLU())\n",
    "\n",
    "def conv22d(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        conv2d(2, in_dim, out_dim),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.LeakyReLU())\n",
    "\n",
    "def conv32d(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        conv2d(1, in_dim, out_dim),\n",
    "        nn.Softmax(dim=1))\n",
    "        # nn.Sigmoid())\n",
    "\n",
    "def tran22d(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        tran2d(2, in_dim, out_dim),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.LeakyReLU())\n",
    "\n",
    "def concat2d(layer1, layer2):\n",
    "    return torch.cat([layer1, layer2], dim=1)\n",
    "\n",
    "class unet2D_new(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(unet2D_new, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        # Contracting layers\n",
    "        self.l1 = conv12d(in_dim, 8)\n",
    "        self.l2 = nn.Sequential(conv22d(8, 16), conv12d(16, 16))\n",
    "        self.l3 = nn.Sequential(conv22d(16, 32), conv12d(32, 32))\n",
    "        self.l4 = nn.Sequential(conv22d(32, 64), conv12d(64, 64))\n",
    "        self.l5 = nn.Sequential(conv22d(64, 128), conv12d(128, 128))\n",
    "\n",
    "        # Expanding layers\n",
    "        self.l6 = tran22d(128, 64)\n",
    "        self.l7 = nn.Sequential(conv12d(128, 64), tran22d(64, 32))\n",
    "        self.l8 = nn.Sequential(conv12d(64, 32), tran22d(32, 16))\n",
    "        self.l9 = nn.Sequential(conv12d(32, 16), tran22d(16, 8))\n",
    "        self.l10 = nn.Sequential(conv32d(8, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contracting layers\n",
    "        l1_out = self.l1(x)\n",
    "        l2_out = self.l2(l1_out)\n",
    "        l3_out = self.l3(l2_out)\n",
    "        l4_out = self.l4(l3_out)\n",
    "        l5_out = self.l5(l4_out)\n",
    "\n",
    "        # Expanding layers\n",
    "        l6_out = self.l6(l5_out)\n",
    "        l7_out = self.l7(concat2d(l4_out, l6_out))\n",
    "        l8_out = self.l8(concat2d(l3_out, l7_out))\n",
    "        l9_out = self.l9(concat2d(l2_out, l8_out))\n",
    "        out = self.l10(l9_out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in size: torch.Size([1, 1, 96, 96])\n",
      "out size: torch.Size([1, 2, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "image_size = 96\n",
    "x = torch.Tensor(1, 1, image_size, image_size)\n",
    "# x.to(device)\n",
    "print(\"in size: {}\".format(x.size()))\n",
    "\n",
    "m_org = unet2D_new(1, 2)\n",
    "o = m_org(x)\n",
    "print(\"out size: {}\".format(o.size()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 3D U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(stride, in_dim, out_dim):\n",
    "    return nn.Conv3d(in_dim, out_dim, kernel_size=3, stride=stride, padding=1)\n",
    "\n",
    "def tran(stride, in_dim, out_dim):\n",
    "    return nn.ConvTranspose3d(in_dim, out_dim, kernel_size=3, stride=stride, padding=1, output_padding=1)\n",
    "\n",
    "def conv1(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        conv(1, in_dim, out_dim),\n",
    "        nn.BatchNorm3d(out_dim),\n",
    "        nn.LeakyReLU())\n",
    "\n",
    "def conv2(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        conv(2, in_dim, out_dim),\n",
    "        nn.BatchNorm3d(out_dim),\n",
    "        nn.LeakyReLU())\n",
    "\n",
    "def conv3(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        conv(1, in_dim, out_dim),\n",
    "        nn.Softmax(dim=1))\n",
    "        # nn.Sigmoid())\n",
    "\n",
    "def tran2(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        tran(2, in_dim, out_dim),\n",
    "        nn.BatchNorm3d(out_dim),\n",
    "        nn.LeakyReLU())\n",
    "\n",
    "def concat(layer1, layer2):\n",
    "    return torch.cat([layer1, layer2], dim=1)\n",
    "\n",
    "class unet3D_new(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(unet3D_new, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        # Contracting layers\n",
    "        self.l1 = conv1(in_dim, 8)\n",
    "        self.l2 = nn.Sequential(conv2(8, 16), conv1(16, 16))\n",
    "        self.l3 = nn.Sequential(conv2(16, 32), conv1(32, 32))\n",
    "        self.l4 = nn.Sequential(conv2(32, 64), conv1(64, 64))\n",
    "        self.l5 = nn.Sequential(conv2(64, 128), conv1(128, 128))\n",
    "\n",
    "        # Expanding layers\n",
    "        self.l6 = tran2(128, 64)\n",
    "        self.l7 = nn.Sequential(conv1(128, 64), tran2(64, 32))\n",
    "        self.l8 = nn.Sequential(conv1(64, 32), tran2(32, 16))\n",
    "        self.l9 = nn.Sequential(conv1(32, 16), tran2(16, 8))\n",
    "        self.l10 = nn.Sequential(conv3(8, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contracting layers\n",
    "        l1_out = self.l1(x)\n",
    "        l2_out = self.l2(l1_out)\n",
    "        l3_out = self.l3(l2_out)\n",
    "        l4_out = self.l4(l3_out)\n",
    "        l5_out = self.l5(l4_out)\n",
    "\n",
    "        # Expanding layers\n",
    "        l6_out = self.l6(l5_out)\n",
    "        l7_out = self.l7(concat(l4_out, l6_out))\n",
    "        l8_out = self.l8(concat(l3_out, l7_out))\n",
    "        l9_out = self.l9(concat(l2_out, l8_out))\n",
    "        out = self.l10(l9_out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in size: torch.Size([1, 1, 96, 96, 96])\n",
      "out size: torch.Size([1, 2, 96, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "image_size = 96\n",
    "x = torch.Tensor(1, 1, image_size, image_size, image_size).cuda()\n",
    "x.to(device)\n",
    "print(\"in size: {}\".format(x.size()))\n",
    "\n",
    "m = unet3D_new(1, 2).cuda()\n",
    "o = m(x)\n",
    "print(\"out size: {}\".format(o.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A working DT\n",
    "def compute_dtm_loop(img_gt, out_shape):\n",
    "    \"\"\"\n",
    "    compute the distance transform map of foreground in binary mask\n",
    "    input: segmentation, shape = (batch_size, num_channels, x, y, z)\n",
    "    output: the foreground Distance Map (SDM) \n",
    "    dtm(x) = 0; x in segmentation boundary\n",
    "             inf|x-y|; x in segmentation\n",
    "    \"\"\"\n",
    "\n",
    "    fg_dtm = np.zeros(out_shape)\n",
    "\n",
    "    for b in range(out_shape[0]): # each batch\n",
    "        for c in range(out_shape[1]): # each channel\n",
    "            # 3D DT\n",
    "            posmask = img_gt[b][c]\n",
    "            if posmask.any():\n",
    "                posdis = distance(1 - posmask)\n",
    "                fg_dtm[b][c] = posdis\n",
    "\n",
    "    return fg_dtm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not in loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size = 4, Scipy_cpu = 0.09ms, Scipy_gpu = 0.28ms\n",
      "size = 529, Scipy_cpu = 0.12ms, Scipy_gpu = 0.31ms\n",
      "size = 1936, Scipy_cpu = 0.20ms, Scipy_gpu = 0.40ms\n",
      "size = 4225, Scipy_cpu = 0.33ms, Scipy_gpu = 0.56ms\n",
      "size = 7569, Scipy_cpu = 0.53ms, Scipy_gpu = 0.75ms\n",
      "size = 11664, Scipy_cpu = 0.77ms, Scipy_gpu = 1.17ms\n",
      "size = 16641, Scipy_cpu = 1.06ms, Scipy_gpu = 1.56ms\n",
      "size = 22500, Scipy_cpu = 1.39ms, Scipy_gpu = 1.97ms\n",
      "size = 29584, Scipy_cpu = 1.80ms, Scipy_gpu = 2.48ms\n",
      "size = 37249, Scipy_cpu = 2.35ms, Scipy_gpu = 3.44ms\n",
      "size = 45796, Scipy_cpu = 2.89ms, Scipy_gpu = 4.10ms\n",
      "size = 55225, Scipy_cpu = 3.47ms, Scipy_gpu = 4.80ms\n",
      "size = 66049, Scipy_cpu = 4.50ms, Scipy_gpu = 5.87ms\n",
      "size = 77284, Scipy_cpu = 5.29ms, Scipy_gpu = 6.78ms\n",
      "size = 89401, Scipy_cpu = 6.01ms, Scipy_gpu = 7.71ms\n",
      "size = 102400, Scipy_cpu = 7.15ms, Scipy_gpu = 10.76ms\n",
      "size = 116964, Scipy_cpu = 8.20ms, Scipy_gpu = 10.16ms\n",
      "size = 131769, Scipy_cpu = 13.68ms, Scipy_gpu = 13.04ms\n",
      "size = 147456, Scipy_cpu = 13.44ms, Scipy_gpu = 14.35ms\n",
      "size = 164025, Scipy_cpu = 14.82ms, Scipy_gpu = 15.95ms\n",
      "size = 182329, Scipy_cpu = 16.19ms, Scipy_gpu = 17.99ms\n",
      "size = 200704, Scipy_cpu = 17.96ms, Scipy_gpu = 19.64ms\n",
      "size = 219961, Scipy_cpu = 19.99ms, Scipy_gpu = 21.57ms\n",
      "size = 240100, Scipy_cpu = 21.93ms, Scipy_gpu = 23.30ms\n",
      "size = 262144, Scipy_cpu = 25.48ms, Scipy_gpu = 27.46ms\n"
     ]
    }
   ],
   "source": [
    "not_loop_2d_Scipy_cpu_hd_cpu_min = []\n",
    "not_loop_2d_Scipy_cpu_hd_cpu_std = []\n",
    "\n",
    "not_loop_2d_Scipy_cpu_hd_gpu_min = []\n",
    "not_loop_2d_Scipy_cpu_hd_gpu_std = []\n",
    "\n",
    "for (idx, case) in enumerate(test_cases_2D):\n",
    "    pred, mask = copy.deepcopy(case)\n",
    "    # cpu -> gpu\n",
    "    pred_gpu, mask_gpu = pred.cuda(), mask.cuda()\n",
    "    \n",
    "    #---Start testing---#\n",
    "\n",
    "    # tfm = Scipy, tfm_device = cpu, hd_device = cpu\n",
    "    Scipy_cpu_hd_cpu = []\n",
    "    for eval in range(1000):\n",
    "        t1 = time.perf_counter_ns()\n",
    "        # DT\n",
    "        pred_dtm = torch.from_numpy(distance(pred))\n",
    "        mask_dtm = torch.from_numpy(distance(mask))\n",
    "        loss = hd_loss_2D(pred, mask, pred_dtm, mask_dtm)\n",
    "        # Loss\n",
    "        t2 = time.perf_counter_ns()\n",
    "        Scipy_cpu_hd_cpu.append(t2-t1)\n",
    "    # tfm = Scipy, tfm_device = cpu, hd_device = gpu\n",
    "    Scipy_cpu_hd_gpu = []\n",
    "    for eval in range(1000):\n",
    "        t1 = time.perf_counter_ns()\n",
    "        # DT\n",
    "        pred_cpu, mask_cpu = pred_gpu.to('cpu'), mask_gpu.to('cpu')\n",
    "        pred_dtm = torch.from_numpy(distance(pred_cpu))\n",
    "        mask_dtm = torch.from_numpy(distance(mask_cpu))\n",
    "        pred_dtm_gpu, mask_dtm_gpu = pred_dtm.cuda(), mask_dtm.cuda()\n",
    "        loss = hd_loss_2D(pred_gpu, mask_gpu, pred_dtm_gpu, mask_dtm_gpu)\n",
    "        # Loss\n",
    "        t2 = time.perf_counter_ns()\n",
    "        Scipy_cpu_hd_gpu.append(t2-t1)\n",
    "\n",
    "    #---Finished testing---#\n",
    "    not_loop_2d_Scipy_cpu_hd_cpu_min.append(np.min(Scipy_cpu_hd_cpu))\n",
    "    not_loop_2d_Scipy_cpu_hd_cpu_std.append(np.std(Scipy_cpu_hd_cpu))\n",
    "    \n",
    "    not_loop_2d_Scipy_cpu_hd_gpu_min.append(np.min(Scipy_cpu_hd_gpu))\n",
    "    not_loop_2d_Scipy_cpu_hd_gpu_std.append(np.std(Scipy_cpu_hd_gpu))\n",
    "\n",
    "    print(\"size = {}, Scipy_cpu = {:0.2f}ms, Scipy_gpu = {:0.2f}ms\".format(sizes_2D[idx], not_loop_2d_Scipy_cpu_hd_cpu_min[-1]*1e-6, not_loop_2d_Scipy_cpu_hd_gpu_min[-1]*1e-6))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2D = {'size': sizes_2D, \n",
    "'Scipy_cpu_hd_cpu_min': not_loop_2d_Scipy_cpu_hd_cpu_min, \n",
    "'Scipy_cpu_hd_cpu_std': not_loop_2d_Scipy_cpu_hd_cpu_std, \n",
    "'Scipy_cpu_hd_gpu_min': not_loop_2d_Scipy_cpu_hd_gpu_min, \n",
    "'Scipy_cpu_hd_gpu_std': not_loop_2d_Scipy_cpu_hd_gpu_std}\n",
    "dataframe2D = pd.DataFrame(data2D)\n",
    "dataframe2D.to_csv(\"C:/Users/wenbl13/Desktop/Ashwin-Timing/distance-transforms/HD_2D_Not_Loop_Jan_12.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size = 8, Scipy_cpu = 0.10ms, Scipy_gpu = 0.35ms\n",
      "size = 343, Scipy_cpu = 0.13ms, Scipy_gpu = 0.41ms\n",
      "size = 1728, Scipy_cpu = 0.26ms, Scipy_gpu = 0.59ms\n",
      "size = 4913, Scipy_cpu = 0.56ms, Scipy_gpu = 0.95ms\n",
      "size = 12167, Scipy_cpu = 1.24ms, Scipy_gpu = 1.72ms\n",
      "size = 21952, Scipy_cpu = 2.16ms, Scipy_gpu = 3.13ms\n",
      "size = 35937, Scipy_cpu = 3.70ms, Scipy_gpu = 4.66ms\n",
      "size = 54872, Scipy_cpu = 6.25ms, Scipy_gpu = 7.33ms\n",
      "size = 85184, Scipy_cpu = 9.94ms, Scipy_gpu = 11.40ms\n",
      "size = 117649, Scipy_cpu = 15.77ms, Scipy_gpu = 17.06ms\n",
      "size = 157464, Scipy_cpu = 22.83ms, Scipy_gpu = 23.17ms\n",
      "size = 205379, Scipy_cpu = 30.72ms, Scipy_gpu = 30.66ms\n",
      "size = 274625, Scipy_cpu = 42.17ms, Scipy_gpu = 42.69ms\n",
      "size = 343000, Scipy_cpu = 54.18ms, Scipy_gpu = 53.19ms\n",
      "size = 421875, Scipy_cpu = 67.63ms, Scipy_gpu = 65.48ms\n",
      "size = 512000, Scipy_cpu = 84.37ms, Scipy_gpu = 81.47ms\n",
      "size = 636056, Scipy_cpu = 103.28ms, Scipy_gpu = 100.10ms\n",
      "size = 753571, Scipy_cpu = 121.49ms, Scipy_gpu = 118.08ms\n",
      "size = 884736, Scipy_cpu = 157.55ms, Scipy_gpu = 149.59ms\n",
      "size = 1030301, Scipy_cpu = 170.84ms, Scipy_gpu = 163.37ms\n",
      "size = 1225043, Scipy_cpu = 205.61ms, Scipy_gpu = 199.43ms\n",
      "size = 1404928, Scipy_cpu = 242.97ms, Scipy_gpu = 225.36ms\n",
      "size = 1601613, Scipy_cpu = 271.41ms, Scipy_gpu = 256.16ms\n",
      "size = 1815848, Scipy_cpu = 296.62ms, Scipy_gpu = 291.87ms\n",
      "size = 2097152, Scipy_cpu = 448.31ms, Scipy_gpu = 451.95ms\n"
     ]
    }
   ],
   "source": [
    "not_loop_3d_Scipy_cpu_hd_cpu_min = []\n",
    "not_loop_3d_Scipy_cpu_hd_cpu_std = []\n",
    "\n",
    "not_loop_3d_Scipy_cpu_hd_gpu_min = []\n",
    "not_loop_3d_Scipy_cpu_hd_gpu_std = []\n",
    "\n",
    "for (idx, case) in enumerate(test_cases_3D):\n",
    "    pred, mask = copy.deepcopy(case)\n",
    "    # cpu -> gpu\n",
    "    pred_gpu, mask_gpu = pred.cuda(), mask.cuda()\n",
    "    \n",
    "    #---Start testing---#\n",
    "\n",
    "    # tfm = Scipy, tfm_device = cpu, hd_device = cpu\n",
    "    Scipy_cpu_hd_cpu = []\n",
    "    for eval in range(1000):\n",
    "        t1 = time.perf_counter_ns()\n",
    "        # DT\n",
    "        pred_dtm = torch.from_numpy(distance(pred))\n",
    "        mask_dtm = torch.from_numpy(distance(mask))\n",
    "        loss = hd_loss_3D(pred, mask, pred_dtm, mask_dtm)\n",
    "        # Loss\n",
    "        t2 = time.perf_counter_ns()\n",
    "        Scipy_cpu_hd_cpu.append(t2-t1)\n",
    "    # tfm = Scipy, tfm_device = cpu, hd_device = gpu\n",
    "    Scipy_cpu_hd_gpu = []\n",
    "    for eval in range(1000):\n",
    "        t1 = time.perf_counter_ns()\n",
    "        # DT\n",
    "        pred_cpu, mask_cpu = pred_gpu.to('cpu'), mask_gpu.to('cpu')\n",
    "        pred_dtm = torch.from_numpy(distance(pred_cpu))\n",
    "        mask_dtm = torch.from_numpy(distance(mask_cpu))\n",
    "        pred_dtm_gpu, mask_dtm_gpu = pred_dtm.cuda(), mask_dtm.cuda()\n",
    "        loss = hd_loss_3D(pred_gpu, mask_gpu, pred_dtm_gpu, mask_dtm_gpu)\n",
    "        # Loss\n",
    "        t2 = time.perf_counter_ns()\n",
    "        Scipy_cpu_hd_gpu.append(t2-t1)\n",
    "\n",
    "    #---Finished testing---#\n",
    "    not_loop_3d_Scipy_cpu_hd_cpu_min.append(np.min(Scipy_cpu_hd_cpu))\n",
    "    not_loop_3d_Scipy_cpu_hd_cpu_std.append(np.std(Scipy_cpu_hd_cpu))\n",
    "\n",
    "    not_loop_3d_Scipy_cpu_hd_gpu_min.append(np.min(Scipy_cpu_hd_gpu))\n",
    "    not_loop_3d_Scipy_cpu_hd_gpu_std.append(np.std(Scipy_cpu_hd_gpu))\n",
    "\n",
    "    print(\"size = {}, Scipy_cpu = {:0.2f}ms, Scipy_gpu = {:0.2f}ms\".format(sizes_3D[idx], not_loop_3d_Scipy_cpu_hd_cpu_min[-1]*1e-6, not_loop_3d_Scipy_cpu_hd_gpu_min[-1]*1e-6))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3D = {'size': sizes_3D, \n",
    "'Scipy_cpu_hd_cpu_min': not_loop_3d_Scipy_cpu_hd_cpu_min, \n",
    "'Scipy_cpu_hd_cpu_std': not_loop_3d_Scipy_cpu_hd_cpu_std, \n",
    "'Scipy_cpu_hd_gpu_min': not_loop_3d_Scipy_cpu_hd_gpu_min, \n",
    "'Scipy_cpu_hd_gpu_std': not_loop_3d_Scipy_cpu_hd_gpu_std}\n",
    "dataframe3D = pd.DataFrame(data3D)\n",
    "dataframe3D.to_csv(\"C:/Users/wenbl13/Desktop/Ashwin-Timing/distance-transforms/HD_3D_Not_Loop_Jan_12.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d9ef0e181354117f9ce70876735363e58fcc077c1120e406476deb8970979d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
